{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MFCC feature extraction and Network training\n",
    "\n",
    "In this notebook you will go through an example flow of processing audio data, complete with feature extraction and training.\n",
    "\n",
    "Make sure you read the instructions on the exercise sheet and follow the task order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from scipy.io import wavfile\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, regularizers\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "DataSetPath = os.path.join(\"hey_snips_kws_4.0\", \"hey_snips_research_6k_en_train_eval_clean_ter/\")\n",
    "\n",
    "with open(DataSetPath+\"train.json\") as jsonfile:\n",
    "    traindata = json.load(jsonfile)\n",
    "\n",
    "with open(DataSetPath+\"test.json\") as jsonfile:\n",
    "    testdata = json.load(jsonfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    x_train_list = []\n",
    "    y_train_list = []\n",
    "\n",
    "    x_test_list = []\n",
    "    y_test_list = []\n",
    "\n",
    "    totalSliceLength = 10 # Length to stuff the signals to, given in seconds\n",
    "\n",
    "    # trainsize = len(traindata) # Number of loaded training samples\n",
    "    # testsize = len(testdata) # Number of loaded testing samples\n",
    "\n",
    "    trainsize = 1000 # Number of loaded training samples\n",
    "    testsize = 100 # Number of loaded testing samples\n",
    "\n",
    "\n",
    "    fs = 16000 # Sampling rate of the samples\n",
    "    segmentLength = 1024 # Number of samples to use per segment\n",
    "\n",
    "    sliceLength = int(totalSliceLength * fs / segmentLength)*segmentLength\n",
    "\n",
    "    for i in tqdm(range(trainsize)): \n",
    "        fs, train_sound_data = wavfile.read(DataSetPath+traindata[i]['audio_file_path']) # Read wavfile to extract amplitudes\n",
    "\n",
    "        _x_train = train_sound_data.copy() # Get a mutable copy of the wavfile\n",
    "        _x_train.resize(sliceLength) # Zero stuff the single to a length of sliceLength\n",
    "        _x_train = _x_train.reshape(-1,int(segmentLength)) # Split slice into Segments with 0 overlap\n",
    "        x_train_list.append(_x_train.astype(np.float32)) # Add segmented slice to training sample list, cast to float so librosa doesn't complain\n",
    "        y_train_list.append(traindata[i]['is_hotword']) # Read label \n",
    "\n",
    "    for i in tqdm(range(testsize)):\n",
    "        fs, test_sound_data = wavfile.read(DataSetPath+testdata[i]['audio_file_path'])\n",
    "        _x_test = test_sound_data.copy()\n",
    "        _x_test.resize(sliceLength)\n",
    "        _x_test = _x_test.reshape((-1,int(segmentLength)))\n",
    "        x_test_list.append(_x_test.astype(np.float32))\n",
    "        y_test_list.append(testdata[i]['is_hotword'])\n",
    "\n",
    "    x_train = tf.convert_to_tensor(np.asarray(x_train_list))\n",
    "    y_train = tf.convert_to_tensor(np.asarray(y_train_list))\n",
    "\n",
    "    x_test = tf.convert_to_tensor(np.asarray(x_test_list))\n",
    "    y_test = tf.convert_to_tensor(np.asarray(y_test_list))\n",
    "\n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mfccs(tensor):\n",
    "    sample_rate = 16000.0\n",
    "    lower_edge_hertz, upper_edge_hertz, num_mel_bins = 80.0, 7600.0, 64\n",
    "    frame_length = 1024\n",
    "    num_mfcc = 13\n",
    "\n",
    "    stfts = tf.signal.stft(tensor, frame_length=frame_length, frame_step=frame_length, fft_length=frame_length) \n",
    "    spectrograms = tf.abs(stfts)\n",
    "    spectrograms = tf.reshape(spectrograms, (spectrograms.shape[0],spectrograms.shape[1],-1))\n",
    "    num_spectrogram_bins = stfts.shape[-1]\n",
    "    linear_to_mel_weight_matrix = tf.signal.linear_to_mel_weight_matrix(\n",
    "      num_mel_bins, num_spectrogram_bins, sample_rate, lower_edge_hertz,\n",
    "      upper_edge_hertz)\n",
    "    mel_spectrograms = tf.tensordot(spectrograms, linear_to_mel_weight_matrix, 1)\n",
    "    log_mel_spectrograms = tf.math.log(mel_spectrograms + 1e-6)\n",
    "    mfccs = tf.signal.mfccs_from_log_mel_spectrograms(log_mel_spectrograms)[..., :num_mfcc]\n",
    "    return tf.reshape(mfccs, (mfccs.shape[0],mfccs.shape[1],mfccs.shape[2],-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 1260.83it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 1359.52it/s]\n",
      "(1000, 156, 1024)\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train, x_test, y_test = load_data()\n",
    "\n",
    "import pickle\n",
    "with open('x_train.txt', 'wb') as f:\n",
    "    pickle.dump(x_train, f)\n",
    "\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(1000, 156, 13, 1)\n(100, 156, 13, 1)\n"
     ]
    }
   ],
   "source": [
    "x_train_mfcc = compute_mfccs(x_train)\n",
    "x_test_mfcc = compute_mfccs(x_test)\n",
    "\n",
    "print(x_train_mfcc.shape)\n",
    "print(x_test_mfcc.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tf.Tensor(167.86346, shape=(), dtype=float32)\ntf.Tensor(-174.75427, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(tf.reduce_max(x_train_mfcc))\n",
    "print(tf.reduce_min(x_train_mfcc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchSize = 10\n",
    "epochs = 30\n",
    "\n",
    "train_set = (x_train_mfcc/512 + 0.5)\n",
    "train_labels = y_train\n",
    "\n",
    "test_set = (x_test_mfcc/512 + 0.5)\n",
    "test_labels = y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/30\n",
      "90/90 [==============================] - 17s 176ms/step - loss: 0.5578 - accuracy: 0.6515 - val_loss: 0.0961 - val_accuracy: 1.0000\n",
      "Epoch 2/30\n",
      "90/90 [==============================] - 28s 312ms/step - loss: 0.2085 - accuracy: 0.8984 - val_loss: 0.0082 - val_accuracy: 1.0000\n",
      "Epoch 3/30\n",
      "90/90 [==============================] - 23s 258ms/step - loss: 0.1284 - accuracy: 0.9481 - val_loss: 7.4035e-04 - val_accuracy: 1.0000\n",
      "Epoch 4/30\n",
      "90/90 [==============================] - 14s 150ms/step - loss: 0.0938 - accuracy: 0.9612 - val_loss: 1.4544e-07 - val_accuracy: 1.0000\n",
      "Epoch 5/30\n",
      "90/90 [==============================] - 24s 267ms/step - loss: 0.0986 - accuracy: 0.9691 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 6/30\n",
      "90/90 [==============================] - 28s 309ms/step - loss: 0.0617 - accuracy: 0.9832 - val_loss: 1.9645e-04 - val_accuracy: 1.0000\n",
      "Epoch 7/30\n",
      "90/90 [==============================] - 27s 296ms/step - loss: 0.0581 - accuracy: 0.9822 - val_loss: 3.5763e-09 - val_accuracy: 1.0000\n",
      "Epoch 8/30\n",
      "90/90 [==============================] - 20s 223ms/step - loss: 0.0540 - accuracy: 0.9828 - val_loss: 3.2001e-05 - val_accuracy: 1.0000\n",
      "Epoch 9/30\n",
      "90/90 [==============================] - 23s 253ms/step - loss: 0.0619 - accuracy: 0.9720 - val_loss: 0.0057 - val_accuracy: 1.0000\n",
      "Epoch 10/30\n",
      "90/90 [==============================] - 26s 292ms/step - loss: 0.0508 - accuracy: 0.9858 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 11/30\n",
      "90/90 [==============================] - 46s 509ms/step - loss: 0.0588 - accuracy: 0.9807 - val_loss: 3.8743e-07 - val_accuracy: 1.0000\n",
      "Epoch 12/30\n",
      "14/90 [===>..........................] - ETA: 1:07 - loss: 0.0521 - accuracy: 0.9760"
     ]
    }
   ],
   "source": [
    "\n",
    "model = tf.keras.models.Sequential()\n",
    "\n",
    "#model.add(layers.InputLayer(input_shape=(train_set.shape[1],train_set.shape[2],train_set.shape[3]), batch_size=(batchSize)))\n",
    "model.add(layers.Conv2D(filters=3,kernel_size=(3,3),padding=\"same\",input_shape=(train_set[0].shape)))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Activation('relu'))\n",
    "\n",
    "model.add(layers.Conv2D(filters=16,kernel_size=(3,3),strides=(2,2),padding='same'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Activation('relu'))\n",
    "\n",
    "model.add(layers.MaxPool2D((2,2)))\n",
    "\n",
    "model.add(layers.Conv2D(filters=32,kernel_size=(3,3),strides=(2,2),padding='same'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Activation('relu'))\n",
    "\n",
    "model.add(layers.MaxPool2D((2,2)))\n",
    "\n",
    "model.add(layers.Conv2D(filters=48,kernel_size=(3,3),padding='same',strides=(2,2)))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Activation('relu'))\n",
    "\n",
    "model.add(layers.GlobalAveragePooling2D())\n",
    "\n",
    "model.add(layers.Flatten())\n",
    "\n",
    "model.add(layers.Dense(8, kernel_regularizer=(regularizers.l1(0))))\n",
    "model.add(layers.Activation('relu'))\n",
    "\n",
    "model.add(layers.Dense(2))\n",
    "model.add(layers.Activation('softmax'))\n",
    "\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=tf.keras.optimizers.Adam(), metrics=['accuracy'])\n",
    "history = model.fit(train_set, y_train, batchSize, epochs, validation_split=0.1, workers=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 156, 13, 3)        30        \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 156, 13, 3)        12        \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 156, 13, 3)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 78, 7, 16)         448       \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 78, 7, 16)         64        \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 78, 7, 16)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 39, 3, 16)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 20, 2, 32)         4640      \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 20, 2, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 20, 2, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 10, 1, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 5, 1, 48)          13872     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 5, 1, 48)          192       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 5, 1, 48)          0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d (Gl (None, 48)                0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 48)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 8)                 392       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 8)                 0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 18        \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 19,796\n",
      "Trainable params: 19,598\n",
      "Non-trainable params: 198\n",
      "_________________________________________________________________\n",
      "4/4 [==============================] - 0s 23ms/step - loss: 0.0380 - accuracy: 0.9900\n"
     ]
    }
   ],
   "source": [
    "model.summary()\n",
    "score = model.evaluate(test_set, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"MFCCmodel.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TFLite conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpb0sp_d5q/assets\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "29192"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "train_set = train_set.numpy()\n",
    "test_set = test_set.numpy()\n",
    "train_labels = train_labels.numpy()\n",
    "test_labels = test_labels.numpy()\n",
    "tflite_model_name = 'MFCC'\n",
    "# Convert Keras model to a tflite model\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "# Convert the model to the TensorFlow Lite format with quantization\n",
    "quantize = True\n",
    "if (quantize):\n",
    "    def representative_dataset():\n",
    "        for i in range(500):\n",
    "            yield([train_set[i].reshape(1,156,13,1)])\n",
    "    # Set the optimization flag.\n",
    "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "    # Enforce full-int8 quantization\n",
    "    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "    converter.inference_input_type = tf.int8  # or tf.uint8\n",
    "    converter.inference_output_type = tf.int8  # or tf.uint8\n",
    "    # Provide a representative dataset to ensure we quantize correctly.\n",
    "converter.representative_dataset = representative_dataset\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "open(tflite_model_name + '.tflite', 'wb').write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function: Convert some hex value into an array for C programming\n",
    "def hex_to_c_array(hex_data, var_name):\n",
    "\n",
    "    c_str = ''\n",
    "\n",
    "    # Create header guard\n",
    "    c_str += '#ifndef ' + var_name.upper() + '_H\\n'\n",
    "    c_str += '#define ' + var_name.upper() + '_H\\n\\n'\n",
    "\n",
    "    # Add array length at top of file\n",
    "    c_str += '\\nunsigned int ' + var_name + '_len = ' + str(len(hex_data)) + ';\\n'\n",
    "\n",
    "    # Declare C variable\n",
    "    c_str += 'unsigned char ' + var_name + '[] = {'\n",
    "    hex_array = []\n",
    "    for i, val in enumerate(hex_data) :\n",
    "\n",
    "        # Construct string from hex\n",
    "        hex_str = format(val, '#04x')\n",
    "\n",
    "        # Add formatting so each line stays within 80 characters\n",
    "        if (i + 1) < len(hex_data):\n",
    "            hex_str += ','\n",
    "        if (i + 1) % 12 == 0:\n",
    "            hex_str += '\\n '\n",
    "        hex_array.append(hex_str)\n",
    "\n",
    "    # Add closing brace\n",
    "    c_str += '\\n ' + format(' '.join(hex_array)) + '\\n};\\n\\n'\n",
    "\n",
    "    # Close out header guard\n",
    "    c_str += '#endif //' + var_name.upper() + '_H'\n",
    "\n",
    "    return c_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_model_name = 'MFCC'\n",
    "# Write TFLite model to a C source (or header) file\n",
    "with open(c_model_name + '.h', 'w') as file:\n",
    "    file.write(hex_to_c_array(tflite_model, c_model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "== Input details ==\nname: conv2d_input_int8\nshape: [  1 156  13   1]\ntype: <class 'numpy.int8'>\n\n== Output details ==\nname: Identity_int8\nshape: [1 2]\ntype: <class 'numpy.int8'>\n"
     ]
    }
   ],
   "source": [
    "tflite_interpreter = tf.lite.Interpreter(model_path=tflite_model_name + '.tflite')\n",
    "tflite_interpreter.allocate_tensors()\n",
    "input_details = tflite_interpreter.get_input_details()\n",
    "output_details = tflite_interpreter.get_output_details()\n",
    "\n",
    "print(\"== Input details ==\")\n",
    "print(\"name:\", input_details[0]['name'])\n",
    "print(\"shape:\", input_details[0]['shape'])\n",
    "print(\"type:\", input_details[0]['dtype'])\n",
    "\n",
    "print(\"\\n== Output details ==\")\n",
    "print(\"name:\", output_details[0]['name'])\n",
    "print(\"shape:\", output_details[0]['shape'])\n",
    "print(\"type:\", output_details[0]['dtype'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(0.003246503183618188, -128)"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "input_details[0][\"quantization\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.zeros((len(test_set),), dtype=int)\n",
    "input_scale, input_zero_point = input_details[0][\"quantization\"]\n",
    "for i in range(len(test_set)):\n",
    "    val_batch = test_set[i]\n",
    "    val_batch = val_batch / input_scale + input_zero_point\n",
    "    val_batch = np.expand_dims(val_batch, axis=0).astype(input_details[0][\"dtype\"])\n",
    "    tflite_interpreter.set_tensor(input_details[0]['index'], val_batch)\n",
    "    tflite_interpreter.allocate_tensors()\n",
    "    tflite_interpreter.invoke()\n",
    "\n",
    "    tflite_model_predictions = tflite_interpreter.get_tensor(output_details[0]['index'])\n",
    "    #print(\"Prediction results shape:\", tflite_model_predictions.shape)\n",
    "    output = tflite_interpreter.get_tensor(output_details[0]['index'])\n",
    "    predictions[i] = output.argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy of quantized to int8 model is 100.0%\nCompared to float32 accuracy of 98.00000190734863%\nWe have a change of 1.9999980926513672%\n"
     ]
    }
   ],
   "source": [
    "sum = 0\n",
    "for i in range(len(predictions)):\n",
    "    if (predictions[i] == test_labels[i]):\n",
    "        sum = sum + 1\n",
    "accuracy_score = sum / 100\n",
    "print(\"Accuracy of quantized to int8 model is {}%\".format(accuracy_score*100))\n",
    "print(\"Compared to float32 accuracy of {}%\".format(score[1]*100))\n",
    "print(\"We have a change of {}%\".format((accuracy_score-score[1])*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python371064bitmlomclab2conda13886544da5d4eb2a26bf924ee48306c",
   "display_name": "Python 3.7.10 64-bit ('mlomc_lab2': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "metadata": {
   "interpreter": {
    "hash": "747d9c539a0ab7e30c1319cda73980c9fb6a6fbd7c722a0c9aa95dd89f1d1480"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}